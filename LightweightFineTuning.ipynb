{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: QLoRA\n",
    "* Model: bert-base-cased\n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: nyu-mll/glue mrpc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89fd016",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd5b6a1c2444001aa00b8e12ff99af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 649k/649k [00:00<00:00, 2.95MB/s]\n",
      "Downloading data: 100%|██████████| 75.7k/75.7k [00:00<00:00, 1.12MB/s]\n",
      "Downloading data: 100%|██████████| 308k/308k [00:00<00:00, 4.11MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd2d3b67c4340bab133ff9436603ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2e7c09f1d44052bb2ab48f337c3320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0070bc1deec4438391d61f1d974b8171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MRPC validation dataset\n",
    "eval_dataset = load_dataset('nyu-mll/glue', 'mrpc', split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c46bd8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4fb701f9184dd58505536ed4a528df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c3d27b8e324d608ad00dab1a5f2bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d698aa1b32d94304834ee16d1179a0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3cdbde3b544f55bc1092feeced4c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd718478fc44c3588a54efdbef2e534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained base model (BERT) and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1b9050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4221cb7862481eaa5f7408fc19e51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset using a lambda function\n",
    "tokenized_eval_datasets = eval_dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples['sentence1'], \n",
    "        examples['sentence2'], \n",
    "        truncation=True, \n",
    "        padding='max_length'\n",
    "    ), \n",
    "    batched=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019b9f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Evaluation results:\n",
      "{'eval_loss': 0.7258169054985046, 'eval_runtime': 5.2457, 'eval_samples_per_second': 77.777, 'eval_steps_per_second': 4.956}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "base_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_eval_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = base_trainer.evaluate()\n",
    "\n",
    "print(f\"BERT Evaluation results:\\n{eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e84e139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType #, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b278a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MRPC training dataset\n",
    "train_dataset = load_dataset('nyu-mll/glue', 'mrpc', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3662ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9944a65567d14525a45e96449d79d4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de3724f2bac4971b863e50a36c072b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12981f02a6074d36871feb879c33576b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a4468b885e4ff3b723fb4de0f43606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36e5efb9a2f4558b9816dbcec306112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Define BnB Cofiguration for 4-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=False,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "    num_labels=2\n",
    ")\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ae71316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.0.attention.self.query\n",
      "bert.encoder.layer.0.attention.self.key\n",
      "bert.encoder.layer.0.attention.self.value\n",
      "bert.encoder.layer.0.attention.output.dense\n",
      "bert.encoder.layer.0.intermediate.dense\n",
      "bert.encoder.layer.0.output.dense\n",
      "bert.encoder.layer.1.attention.self.query\n",
      "bert.encoder.layer.1.attention.self.key\n",
      "bert.encoder.layer.1.attention.self.value\n",
      "bert.encoder.layer.1.attention.output.dense\n",
      "bert.encoder.layer.1.intermediate.dense\n",
      "bert.encoder.layer.1.output.dense\n",
      "bert.encoder.layer.2.attention.self.query\n",
      "bert.encoder.layer.2.attention.self.key\n",
      "bert.encoder.layer.2.attention.self.value\n",
      "bert.encoder.layer.2.attention.output.dense\n",
      "bert.encoder.layer.2.intermediate.dense\n",
      "bert.encoder.layer.2.output.dense\n",
      "bert.encoder.layer.3.attention.self.query\n",
      "bert.encoder.layer.3.attention.self.key\n",
      "bert.encoder.layer.3.attention.self.value\n",
      "bert.encoder.layer.3.attention.output.dense\n",
      "bert.encoder.layer.3.intermediate.dense\n",
      "bert.encoder.layer.3.output.dense\n",
      "bert.encoder.layer.4.attention.self.query\n",
      "bert.encoder.layer.4.attention.self.key\n",
      "bert.encoder.layer.4.attention.self.value\n",
      "bert.encoder.layer.4.attention.output.dense\n",
      "bert.encoder.layer.4.intermediate.dense\n",
      "bert.encoder.layer.4.output.dense\n",
      "bert.encoder.layer.5.attention.self.query\n",
      "bert.encoder.layer.5.attention.self.key\n",
      "bert.encoder.layer.5.attention.self.value\n",
      "bert.encoder.layer.5.attention.output.dense\n",
      "bert.encoder.layer.5.intermediate.dense\n",
      "bert.encoder.layer.5.output.dense\n",
      "bert.encoder.layer.6.attention.self.query\n",
      "bert.encoder.layer.6.attention.self.key\n",
      "bert.encoder.layer.6.attention.self.value\n",
      "bert.encoder.layer.6.attention.output.dense\n",
      "bert.encoder.layer.6.intermediate.dense\n",
      "bert.encoder.layer.6.output.dense\n",
      "bert.encoder.layer.7.attention.self.query\n",
      "bert.encoder.layer.7.attention.self.key\n",
      "bert.encoder.layer.7.attention.self.value\n",
      "bert.encoder.layer.7.attention.output.dense\n",
      "bert.encoder.layer.7.intermediate.dense\n",
      "bert.encoder.layer.7.output.dense\n",
      "bert.encoder.layer.8.attention.self.query\n",
      "bert.encoder.layer.8.attention.self.key\n",
      "bert.encoder.layer.8.attention.self.value\n",
      "bert.encoder.layer.8.attention.output.dense\n",
      "bert.encoder.layer.8.intermediate.dense\n",
      "bert.encoder.layer.8.output.dense\n",
      "bert.encoder.layer.9.attention.self.query\n",
      "bert.encoder.layer.9.attention.self.key\n",
      "bert.encoder.layer.9.attention.self.value\n",
      "bert.encoder.layer.9.attention.output.dense\n",
      "bert.encoder.layer.9.intermediate.dense\n",
      "bert.encoder.layer.9.output.dense\n",
      "bert.encoder.layer.10.attention.self.query\n",
      "bert.encoder.layer.10.attention.self.key\n",
      "bert.encoder.layer.10.attention.self.value\n",
      "bert.encoder.layer.10.attention.output.dense\n",
      "bert.encoder.layer.10.intermediate.dense\n",
      "bert.encoder.layer.10.output.dense\n",
      "bert.encoder.layer.11.attention.self.query\n",
      "bert.encoder.layer.11.attention.self.key\n",
      "bert.encoder.layer.11.attention.self.value\n",
      "bert.encoder.layer.11.attention.output.dense\n",
      "bert.encoder.layer.11.intermediate.dense\n",
      "bert.encoder.layer.11.output.dense\n",
      "bert.pooler.dense\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "# List the name of all the linear modules of the base model, on which the LoRA configuration will be applied\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4841c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,342,468 || all params: 110,824,708 || trainable%: 1.2113435931633585\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65794aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cf720a0bec448ab9050dd112f3affd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "tokenized_train_datasets = train_dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence1\", \"sentence2\", \"idx\"]\n",
    ")\n",
    "\n",
    "tokenized_train_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3306e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    report_to=None,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f159e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_eval_datasets,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea2f7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2295' max='2295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2295/2295 04:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.615300</td>\n",
       "      <td>0.617240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.594308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.623815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.602974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.612165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2295, training_loss=0.5477208854326235, metrics={'train_runtime': 290.789, 'train_samples_per_second': 63.07, 'train_steps_per_second': 7.892, 'total_flos': 1225251348787200.0, 'train_loss': 0.5477208854326235, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbee37e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5943083167076111,\n",
       " 'eval_runtime': 7.1102,\n",
       " 'eval_samples_per_second': 57.383,\n",
       " 'eval_steps_per_second': 3.657,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the peft trained model before saving it\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3c16470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_lora_model/tokenizer_config.json',\n",
       " 'bert_lora_model/special_tokens_map.json',\n",
       " 'bert_lora_model/vocab.txt',\n",
       " 'bert_lora_model/added_tokens.json',\n",
       " 'bert_lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned LoRA model weights\n",
    "peft_model.save_pretrained(\"bert_lora_model\")\n",
    "tokenizer.save_pretrained(\"bert_lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from peft import AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f40d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Load the saved peft model\n",
    "peft_trained_model = AutoPeftModelForSequenceClassification.from_pretrained(\"bert_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Trainers\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_trained_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_eval_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "peft_results = peft_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Trained Model Evaluation results:\n",
      "{'eval_loss': 0.5943083167076111, 'eval_runtime': 7.0956, 'eval_samples_per_second': 57.5, 'eval_steps_per_second': 3.664}\n"
     ]
    }
   ],
   "source": [
    "print(f\"PEFT Trained Model Evaluation results:\\n{peft_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Base Model</th>\n",
       "      <th>PEFT Trained Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eval Loss</td>\n",
       "      <td>0.725817</td>\n",
       "      <td>0.594308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric  Base Model  PEFT Trained Model\n",
       "0  Eval Loss    0.725817            0.594308"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to compare the results\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Eval Loss\"],\n",
    "        \"Base Model\": [eval_results[\"eval_loss\"]],\n",
    "        \"PEFT Trained Model\": [peft_results[\"eval_loss\"]]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
